{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files merged successfully!\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the CSV files\n",
    "directory = '/home/max/stayahead/out_tmp/pd_scores_pi'\n",
    "\n",
    "# List to hold dataframes\n",
    "dfs = []\n",
    "\n",
    "# Loop through files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all dataframes\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_df.to_csv('/home/max/stayahead/analysis/merged_pd_scores_pi.csv', index=False)\n",
    "\n",
    "print(\"Files merged successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b2b = pd.read_csv('/Users/maxvandenboom/stayahead/bio2byte/1step/bio2byte_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b2b['seq_id'] = df_b2b['seq_id'].str.replace('ACE2-','', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b2b.to_csv('/Users/maxvandenboom/stayahead/bio2byte/1step/bio2byte_metrics_avg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv('/home/max/stayahead/analysis/merged_pd_scores.csv')\n",
    "merged_df['jobs'] = merged_df['jobs'].str.replace('ACE2-ectodomain_and_ACE2-', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.rename(columns={'jobs': 'seq_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('/home/max/stayahead/analysis/merged_pd_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('/home/max/stayahead/snellius2/outputs/analysis/dataset/AF_1step_pre_omicron_04-06.csv')\n",
    "df1 = df1.drop(columns=['site_total_escape', 'mut_escape','normalized_mut_escape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('/home/max/stayahead/analysis/datasets/backup/AF_1step_10-06.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files have been successfully merged and saved as 'af_w_pd_scores.csv'\n"
     ]
    }
   ],
   "source": [
    "# Load the first CSV file\n",
    "df1 = pd.read_csv('/home/max/stayahead/analysis/datasets/predictions/pred_ref/AF_1step_pred_epc_15-06.csv')\n",
    "\n",
    "# Load the second CSV file\n",
    "df2 = pd.read_csv('/home/max/stayahead/analysis/pd_scores_pi_15-06.csv')\n",
    "\n",
    "# Merge the two dataframes on the 'seq_id' column\n",
    "merged_df = pd.merge(df1, df2, on='seq_id', how='inner')\n",
    "\n",
    "# Specify the desired column order\n",
    "column_order = ['seq_id', 'wildtype', 'site', 'mutation', 'rmsd', 'tm_score', 'sasa', 'avg_hydro', 'total_hydro', 'plddt', \n",
    "                'local_net_energy', 'global_net_energy','iptm_ptm', 'iptm', 'pDockQ/mpDockQ', 'average_interface_pae', 'average_interface_plddt', 'binding_energy', \n",
    "                'Num_intf_residues', 'Polar', 'Hydrophobhic', 'Charged', 'contact_pairs', 'sc', 'hb', 'sb', \n",
    "                'int_solv_en', 'int_area', 'pi_score']\n",
    "\n",
    "# Reorder the merged dataframe\n",
    "merged_df = merged_df[column_order]\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_df.to_csv('/home/max/stayahead/analysis/AF_1step_pred_pd_15-06.csv', index=False)\n",
    "\n",
    "print(\"CSV files have been successfully merged and saved as 'af_w_pd_scores.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.read_csv('/home/max/stayahead/analysis/merged_pd_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_jobs = set(score_df['jobs'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlphaFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files have been successfully merged and saved as 'final_merged_file.csv'\n"
     ]
    }
   ],
   "source": [
    "# Load the merged CSV file\n",
    "merged_df = pd.read_csv('/home/max/stayahead/analysis/AF_1step_pred_pd_15-06.csv')\n",
    "\n",
    "# Load the additional CSV file\n",
    "additional_df = pd.read_csv('/home/max/stayahead/analysis/datasets/backup/data_wuhan_v2_formatted.csv')\n",
    "\n",
    "# Merge the dataframes on the 'wildtype', 'site', and 'mutation' columns\n",
    "final_df = pd.merge(merged_df, additional_df, on=['wildtype', 'site', 'mutation'], how='inner')\n",
    "\n",
    "# Specify the desired column order\n",
    "column_order = ['seq_id', 'wildtype', 'site', 'mutation', 'rmsd', 'tm_score', 'sasa', 'avg_hydro', 'total_hydro', 'plddt', \n",
    "                'local_net_energy', 'global_net_energy','iptm_ptm', 'iptm', 'pDockQ/mpDockQ', 'average_interface_pae', 'average_interface_plddt', 'binding_energy', \n",
    "                'Num_intf_residues', 'Polar', 'Hydrophobhic', 'Charged', 'contact_pairs', 'sc', 'hb', 'sb', \n",
    "                'int_solv_en', 'int_area', 'pi_score' ,'bind', 'delta_bind', 'expr', 'delta_expr', 'confidence_bind', 'confidence_expr']\n",
    "\n",
    "# Reorder the final dataframe\n",
    "final_df = final_df[column_order]\n",
    "\n",
    "# Save the final dataframe to a new CSV file\n",
    "final_df.to_csv('AF_1step_pred_wuhan_v2_15-06.csv', index=False)\n",
    "\n",
    "print(\"CSV files have been successfully merged and saved as 'final_merged_file.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the merged CSV file\n",
    "merged_df = pd.read_csv('/Users/maxvandenboom/stayahead/analysis/AF_1step_pred_wuhan_v1_15-06.csv')\n",
    "\n",
    "# Load the additional CSV file\n",
    "additional_df = pd.read_csv('/Users/maxvandenboom/stayahead/bio2byte/1step/bio2byte_metrics_avg.csv')\n",
    "\n",
    "# Merge the dataframes on the 'wildtype', 'site', and 'mutation' columns\n",
    "final_df = pd.merge(merged_df, additional_df, on=['seq_id'], how='inner')\n",
    "\n",
    "# Specify the desired column order\n",
    "column_order = ['seq_id', 'wildtype', 'site', 'mutation', 'rmsd', 'tm_score', 'sasa', 'avg_hydro', 'total_hydro', 'plddt', \n",
    "                'local_net_energy', 'global_net_energy','iptm_ptm', 'iptm', 'pDockQ/mpDockQ', 'average_interface_pae', 'average_interface_plddt', 'binding_energy', \n",
    "                'Num_intf_residues', 'Polar', 'Hydrophobhic', 'Charged', 'contact_pairs', 'sc', 'hb', 'sb', \n",
    "                'int_solv_en', 'int_area', 'pi_score', 'agmata', 'backbone', 'coil', 'disoMine', 'earlyFolding', 'helix', 'ppII', 'sheet', 'sidechain',\n",
    "                'bind', 'delta_bind', 'expr', 'delta_expr', 'confidence_bind', 'confidence_expr']\n",
    "\n",
    "# Reorder the final dataframe\n",
    "final_df = final_df[column_order]\n",
    "\n",
    "# Save the final dataframe to a new CSV file\n",
    "final_df.to_csv('AF_1step_pred_wuhan_v1_b2b.csv', index=False)\n",
    "\n",
    "print(\"CSV files have been successfully merged and saved as 'final_merged_file.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files have been successfully merged and saved as 'final_merged_file.csv'\n"
     ]
    }
   ],
   "source": [
    "# Load the merged CSV file\n",
    "merged_df = pd.read_csv('/home/max/stayahead/analysis/datasets/AF_1step_pred_wuhan_v1_b2b.csv')\n",
    "\n",
    "# Load the additional CSV file\n",
    "additional_df = pd.read_csv('/home/max/stayahead/analysis/datasets/complexity/1-Hot-20-Bits.csv')\n",
    "\n",
    "# Merge the dataframes on the 'wildtype', 'site', and 'mutation' columns\n",
    "final_df = pd.merge(merged_df, additional_df, on=['seq_id'], how='inner')\n",
    "\n",
    "# Specify the desired column order\n",
    "column_order = ['seq_id', 'wildtype', 'site', 'mutation', 'rmsd', 'tm_score', 'sasa', 'avg_hydro', 'total_hydro', 'plddt', \n",
    "                'local_net_energy', 'global_net_energy','iptm_ptm', 'iptm', 'pDockQ/mpDockQ', 'average_interface_pae', 'average_interface_plddt', 'binding_energy', \n",
    "                'Num_intf_residues', 'Polar', 'Hydrophobhic', 'Charged', 'contact_pairs', 'sc', 'hb', 'sb', \n",
    "                'int_solv_en', 'int_area', 'pi_score', 'agmata', 'backbone', 'coil', 'disoMine', 'earlyFolding', 'helix', 'ppII', 'sheet', 'sidechain',\n",
    "                'BDM', 'Shannon Entropy', 'LZW ratio', 'bind', 'delta_bind', 'expr', 'delta_expr', 'confidence_bind', 'confidence_expr']\n",
    "\n",
    "# Reorder the final dataframe\n",
    "final_df = final_df[column_order]\n",
    "\n",
    "# Save the final dataframe to a new CSV file\n",
    "final_df.to_csv('/home/max/stayahead/analysis/datasets/complexity/AF/AF_1step_pred_wuhan_v1_b2b_1_Hot_20_Bits.csv', index=False)\n",
    "\n",
    "print(\"CSV files have been successfully merged and saved as 'final_merged_file.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESMFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files have been successfully merged and saved as 'final_merged_file.csv'\n"
     ]
    }
   ],
   "source": [
    "# Load the merged CSV file\n",
    "merged_df = pd.read_csv('/home/max/stayahead/analysis/ESM_1step_pred.csv')\n",
    "\n",
    "# Load the additional CSV file\n",
    "additional_df = pd.read_csv('/home/max/stayahead/analysis/datasets/backup/data_wuhan_v1_formatted.csv')\n",
    "\n",
    "# Merge the dataframes on the 'wildtype', 'site', and 'mutation' columns\n",
    "final_df = pd.merge(merged_df, additional_df, on=['wildtype', 'site', 'mutation'], how='inner')\n",
    "\n",
    "# Specify the desired column order\n",
    "column_order = ['seq_id', 'wildtype', 'site', 'mutation', 'rmsd', 'tm_score', 'sasa', 'avg_hydro', 'total_hydro', 'plddt', \n",
    "                'local_net_energy', 'global_net_energy', 'bind', 'delta_bind', 'expr', 'delta_expr', 'confidence_bind', 'confidence_expr']\n",
    "\n",
    "# Reorder the final dataframe\n",
    "final_df = final_df[column_order]\n",
    "\n",
    "# Save the final dataframe to a new CSV file\n",
    "final_df.to_csv('/home/max/stayahead/analysis/ESM_1step_pred_wuhan_v1_03-07.csv', index=False)\n",
    "\n",
    "print(\"CSV files have been successfully merged and saved as 'final_merged_file.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files have been successfully merged and saved as 'final_merged_file.csv'\n"
     ]
    }
   ],
   "source": [
    "# Load the merged CSV file\n",
    "merged_df = pd.read_csv('/home/max/stayahead/analysis/ESM_1step_pred_wuhan_v1_03-07.csv')\n",
    "\n",
    "# Load the additional CSV file\n",
    "additional_df = pd.read_csv('/home/max/stayahead/bio2byte/1step/bio2byte_metrics_avg.csv')\n",
    "\n",
    "# Merge the dataframes on the 'wildtype', 'site', and 'mutation' columns\n",
    "final_df = pd.merge(merged_df, additional_df, on=['seq_id'], how='inner')\n",
    "\n",
    "# Specify the desired column order\n",
    "column_order = ['seq_id', 'wildtype', 'site', 'mutation', 'rmsd', 'tm_score', 'sasa', 'avg_hydro', 'total_hydro', 'plddt', \n",
    "                'local_net_energy', 'global_net_energy', 'agmata', 'backbone', 'coil', 'disoMine', 'earlyFolding', 'helix', 'ppII', 'sheet', 'sidechain',\n",
    "                'bind', 'delta_bind', 'expr', 'delta_expr', 'confidence_bind', 'confidence_expr']\n",
    "\n",
    "# Reorder the final dataframe\n",
    "final_df = final_df[column_order]\n",
    "\n",
    "# Save the final dataframe to a new CSV file\n",
    "final_df.to_csv('/home/max/stayahead/analysis/ESM_1step_pred_wuhan_v1_b2b_03-07.csv', index=False)\n",
    "\n",
    "print(\"CSV files have been successfully merged and saved as 'final_merged_file.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files have been successfully merged and saved as 'final_merged_file.csv'\n"
     ]
    }
   ],
   "source": [
    "# Load the merged CSV file\n",
    "merged_df = pd.read_csv('/home/max/stayahead/analysis/ESM_1step_pred_wuhan_v1_b2b_03-07.csv')\n",
    "\n",
    "# Load the additional CSV file\n",
    "additional_df = pd.read_csv('/home/max/stayahead/analysis/datasets/complexity/1-Hot-20-Bits.csv')\n",
    "\n",
    "# Merge the dataframes on the 'wildtype', 'site', and 'mutation' columns\n",
    "final_df = pd.merge(merged_df, additional_df, on=['seq_id'], how='inner')\n",
    "\n",
    "# Specify the desired column order\n",
    "column_order = ['seq_id', 'wildtype', 'site', 'mutation', 'rmsd', 'tm_score', 'sasa', 'avg_hydro', 'total_hydro', 'plddt', \n",
    "                'local_net_energy', 'global_net_energy', 'agmata', 'backbone', 'coil', 'disoMine', 'earlyFolding', 'helix', 'ppII', 'sheet', 'sidechain',\n",
    "                'BDM', 'Shannon Entropy', 'LZW ratio', 'bind', 'delta_bind', 'expr', 'delta_expr', 'confidence_bind', 'confidence_expr']\n",
    "\n",
    "# Reorder the final dataframe\n",
    "final_df = final_df[column_order]\n",
    "\n",
    "# Save the final dataframe to a new CSV file\n",
    "final_df.to_csv('/home/max/stayahead/analysis/datasets/complexity/ESM/ESM_1step_pred_wuhan_v1_b2b_1_Hot_20_Bits.csv', index=False)\n",
    "\n",
    "print(\"CSV files have been successfully merged and saved as 'final_merged_file.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/max/stayahead/analysis/merged_pd_scores_pi.csv')\n",
    "pd_dir = '/home/max/stayahead/out_tmp/pd_output'\n",
    "df_cleaned = df.drop_duplicates(subset=['jobs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd = pd.read_csv('/home/max/stayahead/analysis/merged_pd_scores_pi_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_id</th>\n",
       "      <th>iptm_ptm</th>\n",
       "      <th>iptm</th>\n",
       "      <th>pDockQ/mpDockQ</th>\n",
       "      <th>average_interface_pae</th>\n",
       "      <th>average_interface_plddt</th>\n",
       "      <th>binding_energy</th>\n",
       "      <th>Num_intf_residues</th>\n",
       "      <th>Polar</th>\n",
       "      <th>Hydrophobhic</th>\n",
       "      <th>Charged</th>\n",
       "      <th>contact_pairs</th>\n",
       "      <th>sc</th>\n",
       "      <th>hb</th>\n",
       "      <th>sb</th>\n",
       "      <th>int_solv_en</th>\n",
       "      <th>int_area</th>\n",
       "      <th>pi_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G84L</td>\n",
       "      <td>0.755165</td>\n",
       "      <td>0.726900</td>\n",
       "      <td>0.378981</td>\n",
       "      <td>5.497794</td>\n",
       "      <td>75.705857</td>\n",
       "      <td>-1386.911629</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.095</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.404</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.80</td>\n",
       "      <td>1121.65</td>\n",
       "      <td>-1.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q161C</td>\n",
       "      <td>0.743960</td>\n",
       "      <td>0.713413</td>\n",
       "      <td>0.278492</td>\n",
       "      <td>3.992883</td>\n",
       "      <td>80.915517</td>\n",
       "      <td>-204.153022</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.286</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-2.37</td>\n",
       "      <td>905.39</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N128R</td>\n",
       "      <td>0.739793</td>\n",
       "      <td>0.709483</td>\n",
       "      <td>0.224649</td>\n",
       "      <td>4.269344</td>\n",
       "      <td>81.340636</td>\n",
       "      <td>-281.891071</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.200</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.556</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.40</td>\n",
       "      <td>841.25</td>\n",
       "      <td>-0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L93T</td>\n",
       "      <td>0.724926</td>\n",
       "      <td>0.691477</td>\n",
       "      <td>0.230502</td>\n",
       "      <td>3.990663</td>\n",
       "      <td>80.345138</td>\n",
       "      <td>-127.208822</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.250</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.587</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.27</td>\n",
       "      <td>849.81</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S137L</td>\n",
       "      <td>0.714563</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.235071</td>\n",
       "      <td>4.085076</td>\n",
       "      <td>78.092544</td>\n",
       "      <td>-161.118204</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.250</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.598</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.32</td>\n",
       "      <td>869.33</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  seq_id  iptm_ptm      iptm  pDockQ/mpDockQ  average_interface_pae  \\\n",
       "0   G84L  0.755165  0.726900        0.378981               5.497794   \n",
       "1  Q161C  0.743960  0.713413        0.278492               3.992883   \n",
       "2  N128R  0.739793  0.709483        0.224649               4.269344   \n",
       "3   L93T  0.724926  0.691477        0.230502               3.990663   \n",
       "4  S137L  0.714563  0.679145        0.235071               4.085076   \n",
       "\n",
       "   average_interface_plddt  binding_energy  Num_intf_residues  Polar  \\\n",
       "0                75.705857    -1386.911629               21.0  0.333   \n",
       "1                80.915517     -204.153022                7.0  0.000   \n",
       "2                81.340636     -281.891071                5.0  0.000   \n",
       "3                80.345138     -127.208822                4.0  0.000   \n",
       "4                78.092544     -161.118204                4.0  0.000   \n",
       "\n",
       "   Hydrophobhic  Charged  contact_pairs     sc    hb   sb   int_solv_en  \\\n",
       "0         0.286    0.095           21.0  0.404  10.0  3.0         -5.80   \n",
       "1         0.571    0.286            5.0  0.573   9.0  4.0         -2.37   \n",
       "2         0.400    0.200            4.0  0.556   5.0  3.0         -1.40   \n",
       "3         0.500    0.250            3.0  0.587   5.0  3.0         -1.27   \n",
       "4         0.500    0.250            3.0  0.598   6.0  3.0         -1.32   \n",
       "\n",
       "    int_area pi_score  \n",
       "0    1121.65    -1.22  \n",
       "1     905.39     0.63  \n",
       "2     841.25    -0.18  \n",
       "3     849.81     0.17  \n",
       "4     869.33      0.3  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd = df_pd.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd['jobs'] = df_pd['jobs'].str.replace('ACE2-ectodomain_and_ACE2-', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd = df_pd.rename(columns={'jobs': 'seq_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd = df_pd.drop(columns=['interface'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd.to_csv('/home/max/stayahead/analysis/pd_scores_pi_15-06.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been created.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the JSON file\n",
    "with open('/home/max/stayahead/analysis/datasets/references/bio2byte/BA2_100-[Predictions_in_JSON_format].json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Prepare the data for the CSV\n",
    "records = []\n",
    "\n",
    "# Metrics to exclude\n",
    "exclude_metrics = {'residue_index', 'residue', 'seq'}\n",
    "\n",
    "# Iterate through each sequence in the JSON data\n",
    "for sequence_name, metrics in data.items():\n",
    "    record = {'sequence': sequence_name}\n",
    "    # Calculate the average for each metric, excluding specific ones\n",
    "    for metric, values in metrics.items():\n",
    "        if metric not in exclude_metrics:  # Exclude specified metrics\n",
    "            record[metric] = sum(values) / len(values)\n",
    "    records.append(record)\n",
    "\n",
    "# Create a DataFrame and save it to CSV\n",
    "df = pd.DataFrame(records)\n",
    "df.to_csv('/home/max/stayahead/analysis/datasets/references/bio2byte/b2b_BA2_100.csv', index=False)\n",
    "\n",
    "print(\"CSV file has been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original dataframe from CSV\n",
    "df1 = pd.read_csv('/home/max/stayahead/analysis/datasets/complexity/AF/AMBER_EPC/AF_1step_pred_wuhan_v1_b2b_16_Bits.csv')\n",
    "\n",
    "# Load the dataframe with the column to replace from another CSV\n",
    "df2 = pd.read_csv('/home/max/stayahead/analysis/datasets/backup/AF_epc_parse.csv')\n",
    "\n",
    "# Replace the desired column in the original dataframe with the same column from the second dataframe\n",
    "# Assuming the column name to replace is 'column_to_replace'\n",
    "df1['local_net_energy'] = df2['local_net_energy']\n",
    "df1['global_net_energy'] = df2['global_net_energy']\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "df1.to_csv('/home/max/stayahead/analysis/datasets/complexity/AF/PARSE_EPC/AF_1step_pred_wuhan_v1_b2b_16_Bits.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
