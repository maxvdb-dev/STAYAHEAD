{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "escape_df = pd.read_csv('/home/max/stayahead/snellius2/outputs/analysis/ACE_aggregated_pre_omicron.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "escape_csv = '/home/max/stayahead/evolution/data/escape_data_all.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "escape_df['adjusted_site'] = escape_df['site'].astype(int) - 332\n",
    "# Create a new column by merging the required columns in the specified order\n",
    "escape_df['mut_position'] = escape_df['wildtype'].astype(str) + escape_df['adjusted_site'].astype(str) + escape_df['mutation'].astype(str)\n",
    "\n",
    "# Select only the new column and the 'mut_escape' column\n",
    "new_df = escape_df[['mut_position', 'mut_escape']]\n",
    "\n",
    "# Save the transformed data to a new CSV file\n",
    "new_df.to_csv('escape_all_formatted.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mutations = pd.read_csv('/home/max/stayahead/snellius2/outputs/analysis/ACE_aggregated_pre_omicron.csv')\n",
    "df_sequence = pd.read_csv('/home/max/stayahead/snellius2/outputs/analysis/esmfold/1step/results_ESM_13_05.csv')\n",
    "\n",
    "# Remove 'ACE2-' prefix from 'sequence name'\n",
    "df_sequence['sequence name'] = df_sequence['sequence name'].apply(lambda x: x.replace('ACE2-', ''))\n",
    "\n",
    "# Check for unique matches by dropping duplicate mutations if necessary\n",
    "# This step assumes you want to remove duplicates, and keep the first occurrence\n",
    "df_mutations = df_mutations.drop_duplicates(subset='mut_position', keep='first')\n",
    "\n",
    "# Merge the two dataframes on the sequence name (now corrected) and mut_position\n",
    "merged_df = pd.merge(df_sequence, df_mutations, left_on='sequence name', right_on='mut_position', how='inner')\n",
    "\n",
    "# # Select the relevant columns\n",
    "# final_df = merged_df[['sequence name', 'rmsd', 'tm_score', 'sasa', 'mut_escape']]\n",
    "\n",
    "# # Save the final dataframe to a new CSV file\n",
    "# final_df.to_csv('1step_ESM_ab.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mut_position</th>\n",
       "      <th>mut_escape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1A</td>\n",
       "      <td>0.01629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T1C</td>\n",
       "      <td>0.03804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T1D</td>\n",
       "      <td>0.01622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T1E</td>\n",
       "      <td>0.01515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T1F</td>\n",
       "      <td>0.01187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mut_position  mut_escape\n",
       "0          T1A     0.01629\n",
       "1          T1C     0.03804\n",
       "2          T1D     0.01622\n",
       "3          T1E     0.01515\n",
       "4          T1F     0.01187"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mutations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('/home/max/stayahead/snellius2/outputs/analysis/dataset/tmp/1step_AF_missing.csv')\n",
    "df_2 = pd.read_csv('/home/max/stayahead/snellius2/outputs/analysis/dataset/tmp/epc_AF_missing.csv')\n",
    "columns_to_add = ['local_net_energy', 'global_net_energy']\n",
    "merged_df = pd.merge(df_1, df_2[['seq_id'] + columns_to_add], \n",
    "                     on=['seq_id'], how='left')\n",
    "\n",
    "merged_df.to_csv('AF_1step_missing.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_id</th>\n",
       "      <th>wildtype</th>\n",
       "      <th>site</th>\n",
       "      <th>mutation</th>\n",
       "      <th>rmsd</th>\n",
       "      <th>tm_score</th>\n",
       "      <th>sasa</th>\n",
       "      <th>b_factor</th>\n",
       "      <th>local_net_energy</th>\n",
       "      <th>global_net_energy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G72R</td>\n",
       "      <td>G</td>\n",
       "      <td>72</td>\n",
       "      <td>R</td>\n",
       "      <td>19.917002</td>\n",
       "      <td>0.290312</td>\n",
       "      <td>18593.003567</td>\n",
       "      <td>0.231368</td>\n",
       "      <td>174134.567240</td>\n",
       "      <td>174134.567240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D73L</td>\n",
       "      <td>D</td>\n",
       "      <td>73</td>\n",
       "      <td>L</td>\n",
       "      <td>18.559662</td>\n",
       "      <td>0.320001</td>\n",
       "      <td>16551.722910</td>\n",
       "      <td>0.236001</td>\n",
       "      <td>158378.794682</td>\n",
       "      <td>158378.794682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D73P</td>\n",
       "      <td>D</td>\n",
       "      <td>73</td>\n",
       "      <td>P</td>\n",
       "      <td>18.140328</td>\n",
       "      <td>0.258356</td>\n",
       "      <td>16666.825161</td>\n",
       "      <td>0.236381</td>\n",
       "      <td>179512.756284</td>\n",
       "      <td>179512.756284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E74H</td>\n",
       "      <td>E</td>\n",
       "      <td>74</td>\n",
       "      <td>H</td>\n",
       "      <td>18.129641</td>\n",
       "      <td>0.308090</td>\n",
       "      <td>16805.735473</td>\n",
       "      <td>0.233141</td>\n",
       "      <td>193044.249824</td>\n",
       "      <td>193044.249824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E74P</td>\n",
       "      <td>E</td>\n",
       "      <td>74</td>\n",
       "      <td>P</td>\n",
       "      <td>18.439184</td>\n",
       "      <td>0.270016</td>\n",
       "      <td>17045.420919</td>\n",
       "      <td>0.239332</td>\n",
       "      <td>171998.262787</td>\n",
       "      <td>171998.262787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  seq_id wildtype  site mutation       rmsd  tm_score          sasa  b_factor  \\\n",
       "0   G72R        G    72        R  19.917002  0.290312  18593.003567  0.231368   \n",
       "1   D73L        D    73        L  18.559662  0.320001  16551.722910  0.236001   \n",
       "2   D73P        D    73        P  18.140328  0.258356  16666.825161  0.236381   \n",
       "3   E74H        E    74        H  18.129641  0.308090  16805.735473  0.233141   \n",
       "4   E74P        E    74        P  18.439184  0.270016  17045.420919  0.239332   \n",
       "\n",
       "   local_net_energy  global_net_energy  \n",
       "0     174134.567240      174134.567240  \n",
       "1     158378.794682      158378.794682  \n",
       "2     179512.756284      179512.756284  \n",
       "3     193044.249824      193044.249824  \n",
       "4     171998.262787      171998.262787  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first CSV file\n",
    "df1 = pd.read_csv('/home/max/stayahead/snellius2/outputs/analysis/dataset/tmp/AF_1step_missing.csv')\n",
    "\n",
    "# Load the second CSV file\n",
    "df2 = pd.read_csv('/home/max/stayahead/snellius2/outputs/analysis/dataset/ACE_aggregated_pre_omicron.csv')\n",
    "\n",
    "# Select the columns from the second CSV file to be added\n",
    "columns_to_add = ['site_total_escape', 'mut_escape', 'normalized_mut_escape']\n",
    "\n",
    "# Merge the two DataFrames on the shared columns\n",
    "merged_df = pd.merge(df1, df2[['seq_id', 'wildtype', 'site', 'mutation'] + columns_to_add], \n",
    "                     on=['seq_id', 'wildtype', 'mutation'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop(columns=['site_x', 'site_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_df = pd.read_csv('/home/max/stayahead/snellius2/outputs/analysis/dataset/ESM_1step_pre_omicron.csv')\n",
    "fix_df = fix_df.drop(columns=['site_x', 'site_y'])\n",
    "fix_df.to_csv('ESM_1step_pre_omicron.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv('AF_missing_1step_pre_omicron.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('/home/max/stayahead/snellius2/outputs/analysis/ACE_aggregated_pre_omicron.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('/home/max/stayahead/snellius2/outputs/analysis/dataset/tmp/epc_1step_af.csv')\n",
    "df2 = pd.read_csv('/home/max/stayahead/snellius2/outputs/analysis/dataset/tmp/epc_AF_missing.csv')\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "combined_df.to_csv('epc_AF_28_05.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate entries found in 'seq_id' column:\n",
      "     seq_id wildtype mutation       rmsd  tm_score          sasa      plddt  \\\n",
      "197     T1R        T        R  19.302213  0.981345  10363.273891  90.896122   \n",
      "1627    T1V        T        V  19.339498  0.994740  10129.493457  93.548807   \n",
      "1786    T1W        T        W  19.341027  0.994649  10141.712934  93.572668   \n",
      "1868    T1Y        T        Y  19.329296  0.994681  10174.566498  93.589522   \n",
      "2433    T1S        T        S  19.302166  0.994732  10077.148705  93.816555   \n",
      "2981    F6M        F        M  19.299954  0.994742  10143.116657  93.806262   \n",
      "3693    T1R        T        R  19.305759  0.980502  10430.126438  90.459904   \n",
      "3700    T1V        T        V  19.338177  0.994496  10175.704157  93.776756   \n",
      "3704    T1W        T        W  19.342141  0.994542  10172.382422  93.885552   \n",
      "3705    T1Y        T        Y  19.334146  0.994538  10207.607254  93.720093   \n",
      "3707    T1S        T        S  19.305151  0.994648  10150.853496  94.030929   \n",
      "3709    F6M        F        M  19.315388  0.994547  10179.892633  93.842663   \n",
      "\n",
      "      site_total_escape  mut_escape  normalized_mut_escape  local_net_energy  \\\n",
      "197            0.019780    0.547817               0.005120               NaN   \n",
      "1627           0.019780    0.577325               0.005605               NaN   \n",
      "1786           0.019780    1.061517               0.008630               NaN   \n",
      "1868           0.019780    1.257360               0.010747               NaN   \n",
      "2433           0.019780    0.835128               0.007662               NaN   \n",
      "2981           0.005434    0.940191               0.008705               NaN   \n",
      "3693           0.019780    0.547817               0.005120     186622.439826   \n",
      "3700           0.019780    0.577325               0.005605     162339.303922   \n",
      "3704           0.019780    1.061517               0.008630     162672.025217   \n",
      "3705           0.019780    1.257360               0.010747     184817.415133   \n",
      "3707           0.019780    0.835128               0.007662     185258.344970   \n",
      "3709           0.005434    0.940191               0.008705     158599.132323   \n",
      "\n",
      "      global_net_energy  \n",
      "197                 NaN  \n",
      "1627                NaN  \n",
      "1786                NaN  \n",
      "1868                NaN  \n",
      "2433                NaN  \n",
      "2981                NaN  \n",
      "3693      186622.439826  \n",
      "3700      162339.303922  \n",
      "3704      162672.025217  \n",
      "3705      184817.415133  \n",
      "3707      185258.344970  \n",
      "3709      158599.132323  \n"
     ]
    }
   ],
   "source": [
    "combined_df = pd.read_csv('/home/max/stayahead/snellius2/scripts/analysis/AF_1step_CoV2.csv')\n",
    "# Check for duplicate values in the 'seq_id' column\n",
    "duplicates = combined_df[combined_df.duplicated(subset='seq_id', keep=False)]\n",
    "\n",
    "# Print the duplicate entries\n",
    "if not duplicates.empty:\n",
    "    print(\"Duplicate entries found in 'seq_id' column:\")\n",
    "    print(duplicates)\n",
    "else:\n",
    "    print(\"No duplicate entries found in 'seq_id' column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sequence.to_csv('results_ACE2_formatted.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330\n"
     ]
    }
   ],
   "source": [
    "print(len('MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFSNVTWFHAIHVSGTNGTKRFDNPVLPFNDGVYFASTEKSNIIRGWIFGTTLDSKTQSLLIVNNATNVVIKVCEFQFCNDPFLGVYYHKNNKSWMESEFRVYSSANNCTFEYVSQPFLMDLEGKQGNFKNLREFVFKNIDGYFKIYSKHTPINLVRDLPQGFSALEPLVDLPIGINITRFQTLLALHRSYLTPGDSSSGWTAGAAAYYVGYLQPRTFLLKYNENGTITDAVDCALDPLSETKCTLKSFTVEKGIYQTSNFRVQPTESIVRFP'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332\n"
     ]
    }
   ],
   "source": [
    "print(len('MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFSNVTWFHAIHVSGTNGTKRFDNPVLPFNDGVYFASTEKSNIIRGWIFGTTLDSKTQSLLIVNNATNVVIKVCEFQFCNDPFLGVYYHKNNKSWMESEFRVYSSANNCTFEYVSQPFLMDLEGKQGNFKNLREFVFKNIDGYFKIYSKHTPINLVRDLPQGFSALEPLVDLPIGINITRFQTLLALHRSYLTPGDSSSGWTAGAAAYYVGYLQPRTFLLKYNENGTITDAVDCALDPLSETKCTLKSFTVEKGIYQTSNFRVQPTESIVRFPNI'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('/home/max/stayahead/snellius2/outputs/analysis/dataset/AF_1step_pre_omicron.csv')\n",
    "df2 = pd.read_csv('/home/max/stayahead/snellius2/outputs/analysis/dataset/epc_AF_missing.csv')\n",
    "\n",
    "merged_df = pd.merge(df1, df2, on='seq_id', how='left')\n",
    "\n",
    "merged_df['local_net_energy'] = merged_df['local_net_energy_y'].combine_first(merged_df['local_net_energy_x'])\n",
    "merged_df['global_net_energy'] = merged_df['global_net_energy_y'].combine_first(merged_df['global_net_energy_x'])\n",
    "\n",
    "# Drop the temporary columns created by the merge\n",
    "merged_df = merged_df.drop(columns=['local_net_energy_x', 'local_net_energy_y', 'global_net_energy_x', 'global_net_energy_y'])\n",
    "\n",
    "merged_df.to_csv('AF_1step_pre_omicron_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/max/stayahead/snellius2/scripts/analysis/AF_1step_pre_omicron_test.csv')\n",
    "desired_order = [\n",
    "    'seq_id', 'wildtype', 'mutation', 'rmsd', 'tm_score', 'sasa', 'plddt',\n",
    "    'local_net_energy', 'global_net_energy', 'site_total_escape', 'mut_escape', 'normalized_mut_escape'\n",
    "]\n",
    "\n",
    "df = df[desired_order]\n",
    "\n",
    "df.to_csv('AF_1step_pre_omicron_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     seq_id wildtype mutation       rmsd  tm_score          sasa      plddt  \\\n",
      "5     Q166D        Q        D  19.330270  0.994377  10164.036928  93.906526   \n",
      "59    Q166S        Q        S  19.333387  0.994567  10163.245520  94.017028   \n",
      "87    N169H        N        H  19.333128  0.994505  10171.160568  93.863395   \n",
      "91    Q166R        Q        R  19.330455  0.994558  10193.666204  93.951855   \n",
      "191   G164I        G        I  19.332877  0.994434  10189.683096  94.058599   \n",
      "...     ...      ...      ...        ...       ...           ...        ...   \n",
      "3680  Q166P        Q        P  19.333482  0.994391  10151.794097  93.922866   \n",
      "3686  N169G        N        G  19.322332  0.994720  10173.846427  93.930543   \n",
      "3693  P167W        P        W  19.333526  0.994455  10167.528524  93.747010   \n",
      "3731  P167L        P        L  19.332464  0.994514  10202.904746  94.037354   \n",
      "3733  T168H        T        H  19.331401  0.994579  10171.476649  93.949030   \n",
      "\n",
      "      local_net_energy  global_net_energy  site_total_escape  mut_escape  \\\n",
      "5                  NaN                NaN           0.002168    3.762574   \n",
      "59                 NaN                NaN           0.021450    1.319005   \n",
      "87                 NaN                NaN           0.016480    3.606692   \n",
      "91                 NaN                NaN           0.021450    4.412737   \n",
      "191                NaN                NaN           0.021370   10.952287   \n",
      "...                ...                ...                ...         ...   \n",
      "3680               NaN                NaN                NaN         NaN   \n",
      "3686               NaN                NaN           0.016480    4.685434   \n",
      "3693               NaN                NaN                NaN         NaN   \n",
      "3731               NaN                NaN           0.017180    3.653244   \n",
      "3733               NaN                NaN           0.021910    4.941723   \n",
      "\n",
      "      normalized_mut_escape  \n",
      "5                  0.043751  \n",
      "59                 0.012683  \n",
      "87                 0.030826  \n",
      "91                 0.038041  \n",
      "191                0.081128  \n",
      "...                     ...  \n",
      "3680                    NaN  \n",
      "3686               0.037786  \n",
      "3693                    NaN  \n",
      "3731               0.029462  \n",
      "3733               0.046620  \n",
      "\n",
      "[106 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/home/max/stayahead/snellius2/scripts/analysis/AF_1step_pre_omicron_test.csv')\n",
    "missing_energy_rows = df[df[['local_net_energy', 'global_net_energy']].isnull().any(axis=1)]\n",
    "print(missing_energy_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 106 PDB files to /home/max/stayahead/out_tmp/alphafold/missing_energy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# File paths\n",
    "csv_file_path = '/home/max/stayahead/snellius2/scripts/analysis/AF_1step_CoV2_test.csv'  # Replace with the actual path to your CSV file\n",
    "pdb_folder_path = '/home/max/stayahead/out_tmp/alphafold/1step_all'  # Replace with the path to your folder containing PDB files\n",
    "destination_folder_path = '/home/max/stayahead/out_tmp/alphafold/missing_energy'  # Replace with the path to the destination folder\n",
    "\n",
    "# Create destination folder if it does not exist\n",
    "if not os.path.exists(destination_folder_path):\n",
    "    os.makedirs(destination_folder_path)\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Identify rows with missing values in 'local_net_energy' and 'global_net_energy'\n",
    "missing_energy_rows = df[df[['local_net_energy', 'global_net_energy']].isnull().any(axis=1)]\n",
    "\n",
    "# Loop through the identified rows and copy corresponding PDB files\n",
    "for seq_id in missing_energy_rows['seq_id']:\n",
    "    pdb_file_name = f\"ACE2-{seq_id}.pdb\"\n",
    "    pdb_file_path = os.path.join(pdb_folder_path, pdb_file_name)\n",
    "    \n",
    "    if os.path.isfile(pdb_file_path):\n",
    "        shutil.copy(pdb_file_path, destination_folder_path)\n",
    "    else:\n",
    "        print(f\"File not found: {pdb_file_path}\")\n",
    "\n",
    "print(f\"Copied {len(missing_energy_rows)} PDB files to {destination_folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV file saved to /home/max/stayahead/snellius2/scripts/analysis/AF_1step_pre_omicron_28_05.csv\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "original_csv_file_path = '/home/max/stayahead/snellius2/scripts/analysis/AF_1step_pre_omicron_test.csv'  # Replace with the actual path to your original CSV file\n",
    "calculated_csv_file_path = '/home/max/stayahead/snellius2/outputs/analysis/dataset/epc_1step_esm.csv'  # Replace with the actual path to your CSV file with calculated values\n",
    "updated_csv_file_path = '/home/max/stayahead/snellius2/scripts/analysis/AF_1step_pre_omicron_28_05.csv'  # Replace with the path where you want to save the updated CSV file\n",
    "\n",
    "# Read the original CSV file\n",
    "df_original = pd.read_csv(original_csv_file_path)\n",
    "\n",
    "# Read the CSV file with calculated values\n",
    "df_calculated = pd.read_csv(calculated_csv_file_path)\n",
    "\n",
    "# Merge the original dataframe with the calculated values on 'seq_id'\n",
    "df_updated = pd.merge(df_original, df_calculated, on='seq_id', how='left', suffixes=('', '_new'))\n",
    "\n",
    "# Update the columns with the new values where they exist\n",
    "df_updated['local_net_energy'] = df_updated['local_net_energy_new'].combine_first(df_updated['local_net_energy'])\n",
    "df_updated['global_net_energy'] = df_updated['global_net_energy_new'].combine_first(df_updated['global_net_energy'])\n",
    "\n",
    "# Drop the temporary columns used for merging\n",
    "df_updated.drop(columns=['local_net_energy_new', 'global_net_energy_new'], inplace=True)\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "df_updated.to_csv(updated_csv_file_path, index=False)\n",
    "\n",
    "print(f\"Updated CSV file saved to {updated_csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combined file has been saved as 'path_to_combined_file.csv'\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files\n",
    "longer_file = '/home/max/stayahead/snellius2/scripts/analysis/epc_AF_28_05.csv'\n",
    "shorter_file = '/home/max/stayahead/snellius2/outputs/analysis/dataset/epc_AF_missing.csv'\n",
    "\n",
    "# Read the CSV files into DataFrames\n",
    "df_longer = pd.read_csv(longer_file)\n",
    "df_shorter = pd.read_csv(shorter_file)\n",
    "\n",
    "# Identify duplicates based on 'seq_id' column\n",
    "unique_rows = df_shorter[~df_shorter['seq_id'].isin(df_longer['seq_id'])]\n",
    "\n",
    "# Append the unique rows from the shorter file to the longer file\n",
    "df_combined = pd.concat([df_longer, unique_rows], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "df_combined.to_csv('/home/max/stayahead/snellius2/outputs/analysis/dataset/epc_AF_all.csv', index=False)\n",
    "\n",
    "print(\"The combined file has been saved as 'path_to_combined_file.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     seq_id  local_net_energy  global_net_energy\n",
      "2199   T13H     185287.899703      185287.899703\n",
      "2200   T13H     183260.771926      183260.771926\n",
      "2201   A12E     157449.565248      157449.565248\n",
      "2202   A12E     184726.353355      184726.353355\n",
      "2203   T13C     160135.051555      160135.051555\n",
      "...     ...               ...                ...\n",
      "2294   T13V     186257.901661      186257.901661\n",
      "2295   A12S     155525.307949      155525.307949\n",
      "2296   A12S     160276.653120      160276.653120\n",
      "2297   T13G     187300.730639      187300.730639\n",
      "2298   T13G     186349.850886      186349.850886\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "combined_file = '/home/max/stayahead/snellius2/outputs/analysis/dataset/epc_AF_all.csv'\n",
    "\n",
    "# Read the combined CSV file into a DataFrame\n",
    "df_combined = pd.read_csv(combined_file)\n",
    "\n",
    "# Identify duplicate rows based on the 'seq_id' column\n",
    "duplicates = df_combined[df_combined.duplicated(subset='seq_id', keep=False)]\n",
    "\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cleaned file has been saved as 'epc_af_all.csv'\n"
     ]
    }
   ],
   "source": [
    "# Read the combined CSV file into a DataFrame\n",
    "df_combined = pd.read_csv(combined_file)\n",
    "\n",
    "# Identify duplicates based on the 'seq_id' column\n",
    "duplicates = df_combined.duplicated(subset='seq_id', keep='first')\n",
    "\n",
    "# Remove the first instances of duplicate rows\n",
    "df_cleaned = df_combined[~duplicates]\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "cleaned_file = 'epc_af_all.csv'\n",
    "df_cleaned.to_csv(cleaned_file, index=False)\n",
    "\n",
    "print(f\"The cleaned file has been saved as '{cleaned_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
